{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Data Manipulation and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pandas import json_normalize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Text Processing\n",
    "import emoji\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "import joblib\n",
    "\n",
    "# Statistical Utilities\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_proposed = joblib.load('../Semi_supervised_models_12_13_8.joblib')\n",
    "model_baseline = joblib.load('../Initial_Models_12_11_supervised_dropped_5_2.joblib')\n",
    "test_df = pd.read_parquet('../data/final/testing/combined/combined_testing_accounts_ROBERTA_LDA_missing_dropped_2.parquet')\n",
    "train_df = pd.read_parquet('../data/final/labeled/combined/ROBERTA/train_labeled_LDA_missing_dropped.parquet')\n",
    "val_df = pd.read_parquet('../data/final/labeled/combined/ROBERTA/val_labeled_LDA_missing_dropped.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features_LDA = ['user_id', 'username', 'username_uppercase', 'username_lowercase',\n",
    "                'username_numeric', 'username_special', 'username_length', 'username_se', 'is_missing_username',\n",
    "                'screenname', 'screenname_uppercase', 'screenname_lowercase',\n",
    "                'screenname_numeric', 'screenname_special', 'screenname_length',\n",
    "                'screenname_se', 'screenname_emoji', 'screenname_hashtag',\n",
    "                'screenname_word', 'is_missing_screenname', 'description', 'description_length',\n",
    "                'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4',\n",
    "                'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'is_missing_description',  \n",
    "                'user_md_follower', 'user_md_following', 'user_md_follow_ratio',\n",
    "                'user_md_total_post', 'user_md_total_like', 'user_md_verified',\n",
    "                'is_missing_user_metadata', 'post_md_like_mean',\n",
    "                'post_md_like_std', 'post_md_retweet_mean', 'post_md_retweet_std',\n",
    "                'post_md_reply_mean', 'post_md_reply_std',\n",
    "                'is_missing_post_metadata', 'post_text_length_mean', 'post_text_length_std',\n",
    "                'post_sentiment_score_mean', 'post_sentiment_score_std',\n",
    "                'post_sentiment_numeric_mean', 'post_sentiment_numeric_std',\n",
    "                'post_sentiment_numeric_prop_positive',\n",
    "                'post_sentiment_numeric_prop_neutral',\n",
    "                'post_sentiment_numeric_prop_negative', 'is_missing_post_text']\n",
    "\n",
    "feature_sets_LDA = {\n",
    "    'username': ['username_uppercase', 'username_lowercase', 'username_numeric',\n",
    "                 'username_special', 'username_length', 'username_se', 'is_missing_username'],  # Add all username features\n",
    "    'screenname': ['screenname_uppercase', 'screenname_lowercase',\n",
    "                   'screenname_numeric', 'screenname_special', 'screenname_length',\n",
    "                   'screenname_se', 'is_missing_screenname'],  # Add all screenname features\n",
    "    'description': ['description_length', 'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4',\n",
    "                'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'is_missing_description'],  # Add all description features\n",
    "    'user_metadata': ['user_md_follower', 'user_md_following', 'user_md_follow_ratio',\n",
    "                      'user_md_total_post', 'user_md_total_like', 'user_md_verified',\n",
    "                      'is_missing_user_metadata'],  # Add user metadata features\n",
    "    'post': ['post_md_like_mean', 'post_md_like_std', 'post_md_retweet_mean',\n",
    "                      'post_md_retweet_std', 'post_md_reply_mean', 'post_md_reply_std',\n",
    "                      'is_missing_post_metadata', 'post_text_length_mean', 'post_text_length_std', 'post_sentiment_score_mean',\n",
    "                  'post_sentiment_score_std', 'post_sentiment_numeric_mean', 'post_sentiment_numeric_std',\n",
    "                  'post_sentiment_numeric_prop_positive', 'post_sentiment_numeric_prop_neutral',\n",
    "                  'post_sentiment_numeric_prop_negative', 'is_missing_post_text']  # Add post text features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df[features_LDA]  # All feature columns from the training dataset\n",
    "y_train = train_df['label']    # Label column from the training dataset\n",
    "\n",
    "X_test = test_df[features_LDA]  # All feature columns from the validation dataset\n",
    "y_test = test_df['label']    # Label column from the validation dataset\n",
    "\n",
    "X_val = val_df[features_LDA]  # All feature columns from the validation dataset\n",
    "y_val = val_df['label']    # Label column from the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize arrays to accumulate weighted probabilities\n",
    "bot_prob_sum = np.zeros(len(X_test))\n",
    "human_prob_sum = np.zeros(len(X_test))\n",
    "total_weights = np.zeros(len(X_test))  # To normalize the weighted sums\n",
    "\n",
    "# Define completeness threshold for assigning full weights\n",
    "completeness_threshold = 0.75\n",
    "\n",
    "# Initialize array for calibrated models\n",
    "calibrated_models = {}\n",
    "\n",
    "# Initialize a dictionary to store metrics for each individual model\n",
    "individual_model_metrics = {}\n",
    "\n",
    "# Apply Platt's scaling to each model using CalibratedClassifierCV\n",
    "# Change model\n",
    "for feature_name, models in model_baseline.items():\n",
    "    calibrated_model = CalibratedClassifierCV(estimator=models, method='sigmoid', cv='prefit')\n",
    "    calibrated_model.fit(X_train[feature_sets_LDA[feature_name]], y_train)  # Assuming models are already trained\n",
    "    calibrated_models[feature_name] = calibrated_model\n",
    "\n",
    "# Generate predictions for each model using calibrated models\n",
    "for feature_name, models in calibrated_models.items():\n",
    "    feature_columns = feature_sets_LDA[feature_name]\n",
    "\n",
    "    # Calculate feature completeness per instance (user) for X_test\n",
    "    completeness = X_test[feature_columns].notnull().mean(axis=1)\n",
    "\n",
    "    # Get the is_missing indicator for the subset (e.g., user_metadata, post_text)\n",
    "    # is_missing_subset = X_test[f'is_missing_{feature_name}']\n",
    "\n",
    "    # Adjust weights based on completeness and is_missing indicator\n",
    "    weights = np.where(completeness >= completeness_threshold, 1.0, completeness)\n",
    "    # If the subset is missing entirely (is_missing == 1), reduce weight (set it to 0)\n",
    "    # weights = np.where(is_missing_subset == 1, 0.0, weights)\n",
    "\n",
    "    # Predict calibrated probabilities for X_test\n",
    "    probas = models.predict_proba(X_test[feature_columns])\n",
    "\n",
    "    # Accumulate weighted probabilities for bot and human predictions\n",
    "    human_prob_sum += probas[:, 0] * weights  # Human probabilities\n",
    "    bot_prob_sum += probas[:, 1] * weights    # Bot probabilities\n",
    "\n",
    "    # Accumulate total weights for normalization\n",
    "    total_weights += weights\n",
    "\n",
    "    # Make predictions based on probabilities for individual model performance\n",
    "    individual_predictions = np.where(probas[:, 1] > probas[:, 0], True, False)\n",
    "\n",
    "    # Calculate performance metrics for the individual model\n",
    "    accuracy = accuracy_score(y_test, individual_predictions)\n",
    "    precision = precision_score(y_test, individual_predictions, pos_label=True)\n",
    "    recall = recall_score(y_test, individual_predictions, pos_label=True)\n",
    "    f1 = f1_score(y_test, individual_predictions, pos_label=True)\n",
    "    mcc = matthews_corrcoef(y_test, individual_predictions)\n",
    "\n",
    "    # Store metrics for the individual model\n",
    "    individual_model_metrics[feature_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"mcc\": mcc\n",
    "    }\n",
    "\n",
    "# Normalize the weighted probabilities\n",
    "# Avoid division by zero in case no weights were assigned\n",
    "total_weights_safe = np.where(total_weights == 0, 1, total_weights)\n",
    "avg_human_prob = human_prob_sum / total_weights_safe\n",
    "avg_bot_prob = bot_prob_sum / total_weights_safe\n",
    "\n",
    "# Assign final predictions based on aggregated weighted probabilities\n",
    "final_predictions = np.where(avg_bot_prob > avg_human_prob, True, False)\n",
    "\n",
    "# Evaluate the overall ensemble performance\n",
    "ensemble_accuracy = accuracy_score(y_test, final_predictions)\n",
    "ensemble_precision = precision_score(y_test, final_predictions, pos_label=True)\n",
    "ensemble_recall = recall_score(y_test, final_predictions, pos_label=True)\n",
    "ensemble_f1 = f1_score(y_test, final_predictions, pos_label=True)\n",
    "ensemble_mcc = matthews_corrcoef(y_test, final_predictions)\n",
    "\n",
    "# Print evaluation results for the ensemble\n",
    "print(f\"Ensemble Performance:\")\n",
    "print(f'Accuracy: {ensemble_accuracy:.2f}')\n",
    "print(f'Precision: {ensemble_precision:.2f}')\n",
    "print(f'Recall: {ensemble_recall:.2f}')\n",
    "print(f'F1 Score: {ensemble_f1:.2f}')\n",
    "print(f'MCC: {ensemble_mcc:.2f}')\n",
    "\n",
    "# Print evaluation results for each individual model\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "for feature_name, metrics in individual_model_metrics.items():\n",
    "    print(f\"Performance for {feature_name} model:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
