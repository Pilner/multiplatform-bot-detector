{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Data Manipulation and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pandas import json_normalize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Text Processing\n",
    "import emoji\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "import joblib\n",
    "\n",
    "# Statistical Utilities\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Railey\\wkspaces\\Thesis\\multiplatform-bot-detector\\backend\\venv\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Railey\\wkspaces\\Thesis\\multiplatform-bot-detector\\backend\\venv\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features_LDA = ['user_id', 'username', 'username_uppercase', 'username_lowercase',\n",
    "                'username_numeric', 'username_special', 'username_length', 'username_se', 'is_missing_username',\n",
    "                'screenname', 'screenname_uppercase', 'screenname_lowercase',\n",
    "                'screenname_numeric', 'screenname_special', 'screenname_length',\n",
    "                'screenname_se', 'screenname_emoji', 'screenname_hashtag',\n",
    "                'screenname_word', 'is_missing_screenname', 'description', 'description_length',\n",
    "                'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4',\n",
    "                'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'is_missing_description',  \n",
    "                'user_md_follower', 'user_md_following', 'user_md_follow_ratio',\n",
    "                'user_md_total_post', 'user_md_total_like', 'user_md_verified',\n",
    "                'is_missing_user_metadata', 'post_md_like_mean',\n",
    "                'post_md_like_std', 'post_md_retweet_mean', 'post_md_retweet_std',\n",
    "                'post_md_reply_mean', 'post_md_reply_std',\n",
    "                'is_missing_post_metadata', 'post_text_length_mean', 'post_text_length_std',\n",
    "                'post_sentiment_score_mean', 'post_sentiment_score_std',\n",
    "                'post_sentiment_numeric_mean', 'post_sentiment_numeric_std',\n",
    "                'post_sentiment_numeric_prop_positive',\n",
    "                'post_sentiment_numeric_prop_neutral',\n",
    "                'post_sentiment_numeric_prop_negative', 'is_missing_post_text']\n",
    "\n",
    "feature_sets_LDA = {\n",
    "    'username': ['username_uppercase', 'username_lowercase', 'username_numeric',\n",
    "                 'username_special', 'username_length', 'username_se', 'is_missing_username'],  # Add all username features\n",
    "    'screenname': ['screenname_uppercase', 'screenname_lowercase',\n",
    "                   'screenname_numeric', 'screenname_special', 'screenname_length',\n",
    "                   'screenname_se', 'is_missing_screenname'],  # Add all screenname features\n",
    "    'description': ['description_length', 'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4',\n",
    "                'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'is_missing_description'],  # Add all description features\n",
    "    'user_metadata': ['user_md_follower', 'user_md_following', 'user_md_follow_ratio',\n",
    "                      'user_md_total_post', 'user_md_total_like', 'user_md_verified',\n",
    "                      'is_missing_user_metadata'],  # Add user metadata features\n",
    "    'post': ['post_md_like_mean', 'post_md_like_std', 'post_md_retweet_mean',\n",
    "                      'post_md_retweet_std', 'post_md_reply_mean', 'post_md_reply_std',\n",
    "                      'is_missing_post_metadata', 'post_text_length_mean', 'post_text_length_std', 'post_sentiment_score_mean',\n",
    "                  'post_sentiment_score_std', 'post_sentiment_numeric_mean', 'post_sentiment_numeric_std',\n",
    "                  'post_sentiment_numeric_prop_positive', 'post_sentiment_numeric_prop_neutral',\n",
    "                  'post_sentiment_numeric_prop_negative', 'is_missing_post_text']  # Add post text features\n",
    "}\n",
    "\n",
    "dataset_columns = ['user_id', 'username', 'username_uppercase', 'username_lowercase',\n",
    "                  'username_numeric', 'username_special', 'username_length', 'username_se', 'is_missing_username',\n",
    "                  'screenname', 'screenname_uppercase', 'screenname_lowercase',\n",
    "                  'screenname_numeric', 'screenname_special', 'screenname_length',\n",
    "                  'screenname_se', 'screenname_emoji', 'screenname_hashtag',\n",
    "                  'screenname_word', 'is_missing_screenname', 'description', 'description_length',\n",
    "                  'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4',\n",
    "                  'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'is_missing_description',  \n",
    "                  'user_md_follower', 'user_md_following', 'user_md_follow_ratio',\n",
    "                  'user_md_total_post', 'user_md_total_like', 'user_md_verified',\n",
    "                  'is_missing_user_metadata', 'post_md_like_mean',\n",
    "                  'post_md_like_std', 'post_md_retweet_mean', 'post_md_retweet_std',\n",
    "                  'post_md_reply_mean', 'post_md_reply_std',\n",
    "                  'is_missing_post_metadata', 'post_text_length_mean', 'post_text_length_std',\n",
    "                  'post_sentiment_score_mean', 'post_sentiment_score_std',\n",
    "                  'post_sentiment_numeric_mean', 'post_sentiment_numeric_std',\n",
    "                  'post_sentiment_numeric_prop_positive',\n",
    "                  'post_sentiment_numeric_prop_neutral',\n",
    "                  'post_sentiment_numeric_prop_negative', 'is_missing_post_text', 'label']\n",
    "\n",
    "features = features_LDA\n",
    "feature_sets = feature_sets_LDA\n",
    "\n",
    "train_data_unlabeled = pd.read_parquet('../../data/final/unlabeled/combined/combined_unlabeled_accounts_ROBERTA_LDA_missing_dropped.parquet')\n",
    "train_data_labeled = pd.read_parquet('../../data/final/labeled/combined/ROBERTA/train_labeled_LDA_missing_dropped.parquet')\n",
    "test_data = pd.read_parquet('../../data/final/testing/combined/combined_testing_accounts_ROBERTA_LDA_missing_dropped_2.parquet')\n",
    "val_data = pd.read_parquet('../../data/final/labeled/combined/ROBERTA/val_labeled_LDA_missing_dropped.parquet')\n",
    "\n",
    "X_train = train_data_labeled[features]\n",
    "y_train = train_data_labeled['label']\n",
    "\n",
    "X_val = val_data[features]\n",
    "y_val = val_data['label']\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['label']\n",
    "\n",
    "models_1 = joblib.load('../../models/Initial_Models_12_11_supervised_dropped_5_2.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Bot ratio: 0.50, Human ratio: 0.50\n",
      "Bot threshold: 0.75, Human threshold: 0.75\n",
      "0.6\n",
      "   iteration  high_conf_count  avg_confidence  bot_ratio  human_ratio  \\\n",
      "0          1              978        0.775874    0.55726      0.44274   \n",
      "\n",
      "   consistency_rate  \n",
      "0               NaN  \n",
      "Iteration 1 Evaluation Metrics:\n",
      "Accuracy: 0.61\n",
      "Precision: 0.61\n",
      "Recall: 0.63\n",
      "F1 Score: 0.62\n",
      "MCC: 0.23\n",
      "Iteration: 2\n",
      "Bot ratio: 0.56, Human ratio: 0.44\n",
      "Bot threshold: 0.78, Human threshold: 0.72\n",
      "0.62\n",
      "   iteration  high_conf_count  avg_confidence  bot_ratio  human_ratio  \\\n",
      "0          1              978        0.775874   0.557260     0.442740   \n",
      "1          2             1268        0.769916   0.455836     0.544164   \n",
      "\n",
      "   consistency_rate  \n",
      "0               NaN  \n",
      "1          0.771293  \n",
      "Iteration 2 Evaluation Metrics:\n",
      "Accuracy: 0.62\n",
      "Precision: 0.63\n",
      "Recall: 0.60\n",
      "F1 Score: 0.61\n",
      "MCC: 0.24\n",
      "Iteration: 3\n",
      "Bot ratio: 0.46, Human ratio: 0.54\n",
      "Bot threshold: 0.70, Human threshold: 0.80\n",
      "0.64\n",
      "   iteration  high_conf_count  avg_confidence  bot_ratio  human_ratio  \\\n",
      "0          1              978        0.775874   0.557260     0.442740   \n",
      "1          2             1268        0.769916   0.455836     0.544164   \n",
      "2          3             1843        0.755521   0.625610     0.374390   \n",
      "\n",
      "   consistency_rate  \n",
      "0               NaN  \n",
      "1          0.771293  \n",
      "2          0.688009  \n",
      "Iteration 3 Evaluation Metrics:\n",
      "Accuracy: 0.63\n",
      "Precision: 0.62\n",
      "Recall: 0.72\n",
      "F1 Score: 0.66\n",
      "MCC: 0.27\n",
      "Iteration: 4\n",
      "Bot ratio: 0.63, Human ratio: 0.37\n",
      "Bot threshold: 0.82, Human threshold: 0.68\n",
      "0.6599999999999999\n",
      "   iteration  high_conf_count  avg_confidence  bot_ratio  human_ratio  \\\n",
      "0          1              978        0.775874   0.557260     0.442740   \n",
      "1          2             1268        0.769916   0.455836     0.544164   \n",
      "2          3             1843        0.755521   0.625610     0.374390   \n",
      "3          4             2056        0.750675   0.560798     0.439202   \n",
      "\n",
      "   consistency_rate  \n",
      "0               NaN  \n",
      "1          0.771293  \n",
      "2          0.688009  \n",
      "3          0.896401  \n",
      "Iteration 4 Evaluation Metrics:\n",
      "Accuracy: 0.65\n",
      "Precision: 0.62\n",
      "Recall: 0.80\n",
      "F1 Score: 0.70\n",
      "MCC: 0.31\n",
      "Iteration: 5\n",
      "Bot ratio: 0.56, Human ratio: 0.44\n",
      "Bot threshold: 0.85, Human threshold: 0.65\n",
      "0.6799999999999999\n",
      "   iteration  high_conf_count  avg_confidence  bot_ratio  human_ratio  \\\n",
      "0          1              978        0.775874   0.557260     0.442740   \n",
      "1          2             1268        0.769916   0.455836     0.544164   \n",
      "2          3             1843        0.755521   0.625610     0.374390   \n",
      "3          4             2056        0.750675   0.560798     0.439202   \n",
      "4          5             4199        0.744364   0.282686     0.717314   \n",
      "\n",
      "   consistency_rate  \n",
      "0               NaN  \n",
      "1          0.771293  \n",
      "2          0.688009  \n",
      "3          0.896401  \n",
      "4          0.489640  \n",
      "Iteration 5 Evaluation Metrics:\n",
      "Accuracy: 0.64\n",
      "Precision: 0.61\n",
      "Recall: 0.81\n",
      "F1 Score: 0.70\n",
      "MCC: 0.30\n",
      "Iteration: 6\n",
      "Bot ratio: 0.28, Human ratio: 0.72\n",
      "Bot threshold: 0.66, Human threshold: 0.84\n",
      "0.7\n",
      "   iteration  high_conf_count  avg_confidence  bot_ratio  human_ratio  \\\n",
      "0          1              978        0.775874   0.557260     0.442740   \n",
      "1          2             1268        0.769916   0.455836     0.544164   \n",
      "2          3             1843        0.755521   0.625610     0.374390   \n",
      "3          4             2056        0.750675   0.560798     0.439202   \n",
      "4          5             4199        0.744364   0.282686     0.717314   \n",
      "5          6             3625        0.752581   0.315310     0.684690   \n",
      "\n",
      "   consistency_rate  \n",
      "0               NaN  \n",
      "1          0.771293  \n",
      "2          0.688009  \n",
      "3          0.896401  \n",
      "4          0.489640  \n",
      "5          1.000000  \n",
      "High pseudo-label consistency, stopping self-training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "# Existing constants\n",
    "completeness_threshold = 0.75\n",
    "threshold = 0.75\n",
    "pseudo_label_log = []\n",
    "pseudo_label_consistency = {}\n",
    "\n",
    "def track_pseudo_label_quality(pseudo_labeled_data, previous_labels):\n",
    "    # Track metrics for quality assessment\n",
    "    high_conf_data = pseudo_labeled_data[pseudo_labeled_data['confidence'] > 0]\n",
    "    avg_confidence = high_conf_data['confidence'].mean()\n",
    "    label_counts = high_conf_data['predicted_label'].value_counts(normalize=True)\n",
    "    \n",
    "    # Consistency check\n",
    "    if previous_labels is not None:\n",
    "        aligned_labels = previous_labels.reindex(high_conf_data.index)\n",
    "        consistency_rate = (aligned_labels == high_conf_data['predicted_label']).mean()\n",
    "    else:\n",
    "        consistency_rate = np.nan  # First iteration has no prior labels\n",
    "    \n",
    "    # Log metrics\n",
    "    pseudo_label_log.append({\n",
    "        'iteration': len(pseudo_label_log) + 1,\n",
    "        'high_conf_count': len(high_conf_data),\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'bot_ratio': label_counts.get(1, 0),\n",
    "        'human_ratio': label_counts.get(0, 0),\n",
    "        'consistency_rate': consistency_rate\n",
    "    })\n",
    "    \n",
    "    # Update previous_labels for next iteration consistency check\n",
    "    previous_labels = high_conf_data['predicted_label'].copy()\n",
    "    return previous_labels, consistency_rate\n",
    "\n",
    "def dynamic_thresholding(bot_ratio, human_ratio, iteration):\n",
    "    # Aim for a balanced bot-to-human ratio\n",
    "    target_ratio = 0.5\n",
    "    adjustment_factor = max(0.01, 0.03 - iteration * 0.0005)\n",
    "    # Adjustments based on the ratio difference\n",
    "    ratio_difference = abs(bot_ratio - human_ratio)\n",
    "    \n",
    "    # Combination of scaling and exponential adjustment\n",
    "    scaling_factor = 3  # Controls the sensitivity of the ratio difference\n",
    "    adjustment = adjustment_factor * iteration * np.exp(-ratio_difference)\n",
    "    \n",
    "    if bot_ratio > target_ratio:\n",
    "        # Decrease human threshold more aggressively\n",
    "        human_threshold = max(threshold - adjustment, 0.6)\n",
    "        # Increase bot threshold more conservatively\n",
    "        bot_threshold = min(threshold + adjustment, 0.9)\n",
    "    else:\n",
    "        # Decrease bot threshold more aggressively\n",
    "        bot_threshold = max(threshold - adjustment, 0.6)\n",
    "        # Increase human threshold more conservatively\n",
    "        human_threshold = min(threshold + adjustment, 0.9)\n",
    "\n",
    "    return bot_threshold, human_threshold\n",
    "\n",
    "\n",
    "def class_balanced_sampling(data, bot_ratio, human_ratio, target_size):\n",
    "    # Perform stratified sampling to maintain class balance\n",
    "    bot_data = data[data['predicted_label'] == 1]\n",
    "    human_data = data[data['predicted_label'] == 0]\n",
    "\n",
    "    # Determine the number of samples to balance\n",
    "    bot_count = int(target_size * bot_ratio / (bot_ratio + human_ratio))\n",
    "    human_count = target_size - bot_count\n",
    "\n",
    "    sampled_bots = bot_data.sample(n=min(bot_count, len(bot_data)), replace=False)\n",
    "    sampled_humans = human_data.sample(n=min(human_count, len(human_data)), replace=False)\n",
    "\n",
    "    return pd.concat([sampled_bots, sampled_humans])\n",
    "\n",
    "\n",
    "def iterative_self_training(train_data_unlabeled, train_data_labeled, models_1, feature_sets, X_train, y_train, X_test, y_test, dataset_columns, threshold, completeness_threshold):\n",
    "    iteration = 0\n",
    "    improvements = True\n",
    "    previous_confidently_labeled = 0\n",
    "    previous_labels = None\n",
    "    pseudo_labeled_data = train_data_unlabeled.copy()\n",
    "    train_data_unlabeled['is_labeled'] = False\n",
    "    consistency_rate = np.nan\n",
    "    previous_consistency_rate = np.nan\n",
    "\n",
    "    # Initialize the parameters\n",
    "    initial_threshold = 0.6  # Starting threshold\n",
    "    decay_factor = 0.02  # Decay rate (linear)\n",
    "    max_iterations = 10  # Maximum number of iterations\n",
    "\n",
    "    # Initialize threshold for first iteration\n",
    "    current_threshold = initial_threshold\n",
    "\n",
    "\n",
    "    while improvements:\n",
    "        print(f\"Iteration: {iteration + 1}\")\n",
    "\n",
    "        current_threshold = initial_threshold + decay_factor * iteration\n",
    "        if current_threshold < 0.5:  # You can set a minimum threshold\n",
    "            current_threshold = 0.5  # Prevent threshold from going too low\n",
    "\n",
    "        # Initialize arrays for weighted probabilities\n",
    "        bot_prob_sum = np.zeros(len(train_data_unlabeled))\n",
    "        human_prob_sum = np.zeros(len(train_data_unlabeled))\n",
    "        total_weights = np.zeros(len(train_data_unlabeled))\n",
    "        # Only process rows that are not labeled yet\n",
    "        unlabeled_data = train_data_unlabeled[train_data_unlabeled['is_labeled'] == False]\n",
    "        # pseudo_labeled_data = train_data_unlabeled.copy()\n",
    "\n",
    "        for feature_name, model in models_1.items():\n",
    "            feature_columns = feature_sets[feature_name]\n",
    "            completeness = train_data_unlabeled[feature_columns].notnull().mean(axis=1)\n",
    "            # is_missing_subset = train_data_unlabeled[f'is_missing_{feature_name}']\n",
    "            weights = np.where(completeness >= completeness_threshold, 1.0, completeness)\n",
    "            # weights = np.where(is_missing_subset == 1, 0.0, weights)\n",
    "\n",
    "            probas = model.predict_proba(train_data_unlabeled[feature_columns])\n",
    "            human_prob_sum += probas[:, 0] * weights\n",
    "            bot_prob_sum += probas[:, 1] * weights\n",
    "            total_weights += weights\n",
    "\n",
    "        total_weights_safe = np.where(total_weights == 0, 1, total_weights)\n",
    "        avg_human_prob = human_prob_sum / total_weights_safe\n",
    "        avg_bot_prob = bot_prob_sum / total_weights_safe\n",
    "\n",
    "        # Assign pseudo-labels and calculate confidence\n",
    "        newly_labeled_indices = []  # Track newly labeled data points\n",
    "\n",
    "        # Calculate bot-to-human ratio\n",
    "        if iteration == 0:\n",
    "            # Initialize bot-to-human ratio during the first iteration\n",
    "            bot_ratio = 0.5\n",
    "            human_ratio = 0.5\n",
    "        else:\n",
    "            # Calculate bot-to-human ratio based on pseudo-labeled data\n",
    "            label_counts = high_confidence_data['predicted_label'].value_counts(normalize=True)\n",
    "            bot_ratio = label_counts.get(1, 0)\n",
    "            human_ratio = label_counts.get(0, 0)\n",
    "            \n",
    "        # Apply dynamic thresholding\n",
    "        bot_threshold, human_threshold = dynamic_thresholding(bot_ratio, human_ratio, iteration)\n",
    "\n",
    "        print (f\"Bot ratio: {bot_ratio:.2f}, Human ratio: {human_ratio:.2f}\")\n",
    "        print (f\"Bot threshold: {bot_threshold:.2f}, Human threshold: {human_threshold:.2f}\")\n",
    "        \n",
    "        for i in range(len(avg_human_prob)):\n",
    "            # Access the index of the current row in train_data_unlabeled\n",
    "            if not train_data_unlabeled.iloc[i]['is_labeled']:  # Only process unlabeled data\n",
    "                completeness = train_data_unlabeled[feature_sets[feature_name]].notnull().mean(axis=1).iloc[i]\n",
    "                \n",
    "                # Adjust the smoothing factor dynamically based on previous performance\n",
    "                if previous_consistency_rate is not np.nan:\n",
    "                    if previous_consistency_rate > 0.8:\n",
    "                        smoothing_factor = 0.7  # High consistency, increase smoothing\n",
    "                    elif previous_consistency_rate < 0.5:\n",
    "                        smoothing_factor = 0.3  # Low consistency, decrease smoothing\n",
    "                    else:\n",
    "                        smoothing_factor = 0.5\n",
    "                else:\n",
    "                    smoothing_factor = 0.5  # Neutral, default smoothing factor\n",
    "                \n",
    "                if avg_human_prob[i] > avg_bot_prob[i]:\n",
    "                    pseudo_label = 0  # Human\n",
    "                    confidence = smoothing_factor * avg_human_prob[i] + (1 - smoothing_factor) * completeness\n",
    "                    threshold = human_threshold\n",
    "                else:\n",
    "                    pseudo_label = 1  # Bot\n",
    "                    confidence = smoothing_factor * avg_bot_prob[i] + (1 - smoothing_factor) * completeness\n",
    "                    threshold = bot_threshold\n",
    "\n",
    "                # Apply Confidence Smoothing based on consistency\n",
    "                if consistency_rate is not np.nan:  # If there is a consistency rate from the previous iteration\n",
    "                    if consistency_rate > 0.8:  # If model has been highly consistent\n",
    "                        confidence *= 1.2  # Boost confidence by 10%\n",
    "                    elif consistency_rate < 0.5:  # If model has been inconsistent\n",
    "                        confidence *= 0.8  # Reduce confidence by 30%\n",
    "\n",
    "                # Only update labels and confidence for unlabeled data\n",
    "                if confidence > threshold:\n",
    "                    # Update the pseudo-labeled data with confidence and predicted labels\n",
    "                    pseudo_labeled_data.at[train_data_unlabeled.index[i], 'predicted_label'] = pseudo_label\n",
    "                    pseudo_labeled_data.at[train_data_unlabeled.index[i], 'confidence'] = confidence\n",
    "                    train_data_unlabeled.at[train_data_unlabeled.index[i], 'is_labeled'] = True  # Mark as labeled\n",
    "                    newly_labeled_indices.append(train_data_unlabeled.index[i])\n",
    "\n",
    "        if len(newly_labeled_indices) == 0:\n",
    "            print(\"No new data points were labeled in this iteration.\")\n",
    "            break\n",
    "\n",
    "        # print (pseudo_labeled_data[pseudo_labeled_data['confidence'] > 0])\n",
    "\n",
    "        print (current_threshold)\n",
    "        high_confidence_data = pseudo_labeled_data[pseudo_labeled_data['confidence'] > current_threshold]\n",
    "        # high_confidence_data.to_csv(\"../data/final/unlabeled/combined/high_confidence.csv\")\n",
    "\n",
    "        # # Uncertainty sampling: Select most uncertain samples\n",
    "        # uncertainty_scores = np.abs(pseudo_labeled_data['confidence'] - 0.5)\n",
    "        # most_uncertain_indices = uncertainty_scores.nsmallest(len(high_confidence_data)).index\n",
    "        # uncertainty_samples = pseudo_labeled_data.loc[most_uncertain_indices]\n",
    "\n",
    "        # Combine high-confidence and uncertain samples\n",
    "        # combined_data = pd.concat([high_confidence_data, uncertainty_samples])\n",
    "\n",
    "        combined_data = high_confidence_data.copy()\n",
    "\n",
    "        # Apply class-balanced sampling\n",
    "        target_size = len(combined_data)\n",
    "        combined_data = class_balanced_sampling(combined_data, bot_ratio, human_ratio, target_size)\n",
    "\n",
    "        # Print the number of confidently labeled data points\n",
    "        confidently_labeled_count = len(high_confidence_data)\n",
    "\n",
    "        # Track pseudo label consistency\n",
    "        previous_labels, consistency_rate = track_pseudo_label_quality(high_confidence_data, previous_labels)\n",
    "\n",
    "        # Log summary of pseudo-label quality across iterations\n",
    "        quality_df = pd.DataFrame(pseudo_label_log)\n",
    "        print(quality_df)\n",
    "        \n",
    "        # Check improvement and decide if further self-training is beneficial\n",
    "        # if pseudo_label_log[-1]['avg_confidence'] < threshold or pseudo_label_log[-1]['consistency_rate'] < threshold:\n",
    "        #     print(\"Pseudo-label quality threshold reached. Stopping self-training.\")\n",
    "        #     break\n",
    "\n",
    "        if consistency_rate > 0.99:  # If consistency is high, stop self-training\n",
    "            print(\"High pseudo-label consistency, stopping self-training.\")\n",
    "            improvements = False\n",
    "            break\n",
    "\n",
    "        # Augment the training data\n",
    "        augmented_data = combined_data.copy()\n",
    "        augmented_data['label'] = combined_data['predicted_label'].map(lambda x: True if x == 1 else False)\n",
    "        augmented_data = augmented_data.drop(columns=['predicted_label', 'confidence'])\n",
    "        augmented_data = augmented_data[dataset_columns]\n",
    "\n",
    "        # Combine original labeled data with augmented data\n",
    "        user_train_data_labeled_augmented = pd.concat([train_data_labeled, augmented_data], ignore_index=True)\n",
    "        # user_train_data_labeled_augmented.to_parquet(\"../data/final/unlabeled/combined/augmented_data.parquet\")\n",
    "        X_train_augmented = user_train_data_labeled_augmented.drop(columns=['label'])\n",
    "        y_train_augmented = user_train_data_labeled_augmented['label']\n",
    "\n",
    "        # Retrain models with augmented data\n",
    "        for feature_name, model in models_1.items():\n",
    "            feature_columns = feature_sets[feature_name]\n",
    "            model.fit(X_train_augmented[feature_columns], y_train_augmented)\n",
    "\n",
    "        # Validate the performance on the validation set (metrics calculation remains unchanged)\n",
    "\n",
    "        bot_prob_sum = np.zeros(len(X_test))\n",
    "        human_prob_sum = np.zeros(len(X_test))\n",
    "        total_weights = np.zeros(len(X_test))\n",
    "        # calibrated_models = {}\n",
    "\n",
    "        # for feature_name, model in models_1.items():\n",
    "        #     calibrated_model = CalibratedClassifierCV(estimator=model, method='sigmoid', cv='prefit')\n",
    "        #     calibrated_model.fit(X_val[feature_sets[feature_name]], y_val)\n",
    "        #     models_1[feature_name] = calibrated_model\n",
    "\n",
    "        for feature_name, model in models_1.items():\n",
    "            feature_columns = feature_sets[feature_name]\n",
    "            completeness = X_test[feature_columns].notnull().mean(axis=1)\n",
    "            weights = np.where(completeness >= completeness_threshold, 1.0, completeness)\n",
    "            probas = model.predict_proba(X_test[feature_columns])\n",
    "            human_prob_sum += probas[:, 0] * weights\n",
    "            bot_prob_sum += probas[:, 1] * weights\n",
    "            total_weights += weights\n",
    "\n",
    "        total_weights_safe = np.where(total_weights == 0, 1, total_weights)\n",
    "        avg_human_prob = human_prob_sum / total_weights_safe\n",
    "        avg_bot_prob = bot_prob_sum / total_weights_safe\n",
    "        final_predictions = np.where(avg_bot_prob > avg_human_prob, True, False)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, final_predictions)\n",
    "        precision = precision_score(y_test, final_predictions, pos_label=True)\n",
    "        recall = recall_score(y_test, final_predictions, pos_label=True)\n",
    "        f1 = f1_score(y_test, final_predictions, pos_label=True)\n",
    "        mcc = matthews_corrcoef(y_test, final_predictions)\n",
    "\n",
    "        previous_model_accuracy = accuracy  # Example starting accuracy (this could be tracked across iterations)\n",
    "        previous_consistency_rate = consistency_rate  # Example starting consistency rate\n",
    "\n",
    "        print(f\"Iteration {iteration + 1} Evaluation Metrics:\")\n",
    "        print(f'Accuracy: {accuracy:.2f}')\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1 Score: {f1:.2f}')\n",
    "        print(f'MCC: {mcc:.2f}')\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        # Stop if all unlabeled data has been confidently labeled\n",
    "        if confidently_labeled_count == len(train_data_unlabeled):\n",
    "            print(\"All unlabeled data has been confidently labeled.\")\n",
    "            break\n",
    "\n",
    "\n",
    "iterative_self_training(train_data_unlabeled, train_data_labeled, models_1, feature_sets, X_train, y_train, X_test, y_test, dataset_columns, threshold, completeness_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
